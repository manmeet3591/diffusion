{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Copyright (c) 2022, NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n",
        "#\n",
        "# This work is licensed under a Creative Commons\n",
        "# Attribution-NonCommercial-ShareAlike 4.0 International License.\n",
        "# You should have received a copy of the license along with this\n",
        "# work. If not, see http://creativecommons.org/licenses/by-nc-sa/4.0/\n",
        "\n",
        "# This file was modified by Robbie Watt (2024) for the purpose of downscaling\n",
        "# climate data\n",
        "\n",
        "\"\"\"Model architectures and preconditioning schemes used in the paper\n",
        "\"Elucidating the Design Space of Diffusion-Based Generative Models\".\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.nn.functional import silu\n",
        "\n",
        "#----------------------------------------------------------------------------\n",
        "# Unified routine for initializing weights and biases.\n",
        "\n",
        "def weight_init(shape, mode, fan_in, fan_out):\n",
        "    if mode == 'xavier_uniform': return np.sqrt(6 / (fan_in + fan_out)) * (torch.rand(*shape) * 2 - 1)\n",
        "    if mode == 'xavier_normal':  return np.sqrt(2 / (fan_in + fan_out)) * torch.randn(*shape)\n",
        "    if mode == 'kaiming_uniform': return np.sqrt(3 / fan_in) * (torch.rand(*shape) * 2 - 1)\n",
        "    if mode == 'kaiming_normal':  return np.sqrt(1 / fan_in) * torch.randn(*shape)\n",
        "    raise ValueError(f'Invalid init mode \"{mode}\"')\n",
        "\n",
        "#----------------------------------------------------------------------------\n",
        "# Fully-connected layer.\n",
        "\n",
        "class Linear(torch.nn.Module):\n",
        "    def __init__(self, in_features, out_features, bias=True, init_mode='kaiming_normal', init_weight=1, init_bias=0):\n",
        "        super().__init__()\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        init_kwargs = dict(mode=init_mode, fan_in=in_features, fan_out=out_features)\n",
        "        self.weight = torch.nn.Parameter(weight_init([out_features, in_features], **init_kwargs) * init_weight)\n",
        "        self.bias = torch.nn.Parameter(weight_init([out_features], **init_kwargs) * init_bias) if bias else None\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x @ self.weight.to(x.dtype).t()\n",
        "        if self.bias is not None:\n",
        "            x = x.add_(self.bias.to(x.dtype))\n",
        "        return x\n",
        "\n",
        "#----------------------------------------------------------------------------\n",
        "# Convolutional layer with optional up/downsampling.\n",
        "\n",
        "class Conv2d(torch.nn.Module):\n",
        "    def __init__(self,\n",
        "        in_channels, out_channels, kernel, bias=True, up=False, down=False,\n",
        "        resample_filter=[1,1], fused_resample=False, init_mode='kaiming_normal', init_weight=1, init_bias=0,\n",
        "    ):\n",
        "        assert not (up and down)\n",
        "        super().__init__()\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.up = up\n",
        "        self.down = down\n",
        "        self.fused_resample = fused_resample\n",
        "        init_kwargs = dict(mode=init_mode, fan_in=in_channels*kernel*kernel, fan_out=out_channels*kernel*kernel)\n",
        "        self.weight = torch.nn.Parameter(weight_init([out_channels, in_channels, kernel, kernel], **init_kwargs) * init_weight) if kernel else None\n",
        "        self.bias = torch.nn.Parameter(weight_init([out_channels], **init_kwargs) * init_bias) if kernel and bias else None\n",
        "        f = torch.as_tensor(resample_filter, dtype=torch.float32)\n",
        "        f = f.ger(f).unsqueeze(0).unsqueeze(1) / f.sum().square()\n",
        "        self.register_buffer('resample_filter', f if up or down else None)\n",
        "\n",
        "    def forward(self, x):\n",
        "        w = self.weight.to(x.dtype) if self.weight is not None else None\n",
        "        b = self.bias.to(x.dtype) if self.bias is not None else None\n",
        "        f = self.resample_filter.to(x.dtype) if self.resample_filter is not None else None\n",
        "        w_pad = w.shape[-1] // 2 if w is not None else 0\n",
        "        f_pad = (f.shape[-1] - 1) // 2 if f is not None else 0\n",
        "\n",
        "        if self.fused_resample and self.up and w is not None:\n",
        "            x = torch.nn.functional.conv_transpose2d(x, f.mul(4).tile([self.in_channels, 1, 1, 1]), groups=self.in_channels, stride=2, padding=max(f_pad - w_pad, 0))\n",
        "            x = torch.nn.functional.conv2d(x, w, padding=max(w_pad - f_pad, 0))\n",
        "        elif self.fused_resample and self.down and w is not None:\n",
        "            x = torch.nn.functional.conv2d(x, w, padding=w_pad+f_pad)\n",
        "            x = torch.nn.functional.conv2d(x, f.tile([self.out_channels, 1, 1, 1]), groups=self.out_channels, stride=2)\n",
        "        else:\n",
        "            if self.up:\n",
        "                x = torch.nn.functional.conv_transpose2d(x, f.mul(4).tile([self.in_channels, 1, 1, 1]), groups=self.in_channels, stride=2, padding=f_pad)\n",
        "            if self.down:\n",
        "                x = torch.nn.functional.conv2d(x, f.tile([self.in_channels, 1, 1, 1]), groups=self.in_channels, stride=2, padding=f_pad)\n",
        "            if w is not None:\n",
        "                x = torch.nn.functional.conv2d(x, w, padding=w_pad)\n",
        "        if b is not None:\n",
        "            x = x.add_(b.reshape(1, -1, 1, 1))\n",
        "        return x\n",
        "\n",
        "#----------------------------------------------------------------------------\n",
        "# Group normalization.\n",
        "\n",
        "class GroupNorm(torch.nn.Module):\n",
        "    def __init__(self, num_channels, num_groups=32, min_channels_per_group=4, eps=1e-5):\n",
        "        super().__init__()\n",
        "        self.num_groups = min(num_groups, num_channels // min_channels_per_group)\n",
        "        self.eps = eps\n",
        "        self.weight = torch.nn.Parameter(torch.ones(num_channels))\n",
        "        self.bias = torch.nn.Parameter(torch.zeros(num_channels))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.nn.functional.group_norm(x, num_groups=self.num_groups, weight=self.weight.to(x.dtype), bias=self.bias.to(x.dtype), eps=self.eps)\n",
        "        return x\n",
        "\n",
        "#----------------------------------------------------------------------------\n",
        "# Attention weight computation, i.e., softmax(Q^T * K).\n",
        "# Performs all computation using FP32, but uses the original datatype for\n",
        "# inputs/outputs/gradients to conserve memory.\n",
        "\n",
        "class AttentionOp(torch.autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, q, k):\n",
        "        w = torch.einsum('ncq,nck->nqk', q.to(torch.float32), (k / np.sqrt(k.shape[1])).to(torch.float32)).softmax(dim=2).to(q.dtype)\n",
        "        ctx.save_for_backward(q, k, w)\n",
        "        return w\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, dw):\n",
        "        q, k, w = ctx.saved_tensors\n",
        "        db = torch._softmax_backward_data(grad_output=dw.to(torch.float32), output=w.to(torch.float32), dim=2, input_dtype=torch.float32)\n",
        "        dq = torch.einsum('nck,nqk->ncq', k.to(torch.float32), db).to(q.dtype) / np.sqrt(k.shape[1])\n",
        "        dk = torch.einsum('ncq,nqk->nck', q.to(torch.float32), db).to(k.dtype) / np.sqrt(k.shape[1])\n",
        "        return dq, dk\n",
        "\n",
        "#----------------------------------------------------------------------------\n",
        "# Unified U-Net block with optional up/downsampling and self-attention.\n",
        "# Represents the union of all features employed by the DDPM++, NCSN++, and\n",
        "# ADM architectures.\n",
        "\n",
        "class UNetBlock(torch.nn.Module):\n",
        "    def __init__(self,\n",
        "        in_channels, out_channels, emb_channels, up=False, down=False, attention=False,\n",
        "        num_heads=None, channels_per_head=64, dropout=0, skip_scale=1, eps=1e-5,\n",
        "        resample_filter=[1,1], resample_proj=False, adaptive_scale=True,\n",
        "        init=dict(), init_zero=dict(init_weight=0), init_attn=None,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.emb_channels = emb_channels\n",
        "        self.num_heads = 0 if not attention else num_heads if num_heads is not None else out_channels // channels_per_head\n",
        "        self.dropout = dropout\n",
        "        self.skip_scale = skip_scale\n",
        "        self.adaptive_scale = adaptive_scale\n",
        "\n",
        "        self.norm0 = GroupNorm(num_channels=in_channels, eps=eps)\n",
        "        self.conv0 = Conv2d(in_channels=in_channels, out_channels=out_channels, kernel=3, up=up, down=down, resample_filter=resample_filter, **init)\n",
        "        self.affine = Linear(in_features=emb_channels, out_features=out_channels*(2 if adaptive_scale else 1), **init)\n",
        "        self.norm1 = GroupNorm(num_channels=out_channels, eps=eps)\n",
        "        self.conv1 = Conv2d(in_channels=out_channels, out_channels=out_channels, kernel=3, **init_zero)\n",
        "\n",
        "        self.skip = None\n",
        "        if out_channels != in_channels or up or down:\n",
        "            kernel = 1 if resample_proj or out_channels!= in_channels else 0\n",
        "            self.skip = Conv2d(in_channels=in_channels, out_channels=out_channels, kernel=kernel, up=up, down=down, resample_filter=resample_filter, **init)\n",
        "\n",
        "        if self.num_heads:\n",
        "            self.norm2 = GroupNorm(num_channels=out_channels, eps=eps)\n",
        "            self.qkv = Conv2d(in_channels=out_channels, out_channels=out_channels*3, kernel=1, **(init_attn if init_attn is not None else init))\n",
        "            self.proj = Conv2d(in_channels=out_channels, out_channels=out_channels, kernel=1, **init_zero)\n",
        "\n",
        "    def forward(self, x, emb):\n",
        "        orig = x\n",
        "        x = self.conv0(silu(self.norm0(x)))\n",
        "\n",
        "        params = self.affine(emb).unsqueeze(2).unsqueeze(3).to(x.dtype)\n",
        "        if self.adaptive_scale:\n",
        "            scale, shift = params.chunk(chunks=2, dim=1)\n",
        "            x = silu(torch.addcmul(shift, self.norm1(x), scale + 1))\n",
        "        else:\n",
        "            x = silu(self.norm1(x.add_(params)))\n",
        "\n",
        "        x = self.conv1(torch.nn.functional.dropout(x, p=self.dropout, training=self.training))\n",
        "        x = x.add_(self.skip(orig) if self.skip is not None else orig)\n",
        "        x = x * self.skip_scale\n",
        "\n",
        "        if self.num_heads:\n",
        "            q, k, v = self.qkv(self.norm2(x)).reshape(x.shape[0] * self.num_heads, x.shape[1] // self.num_heads, 3, -1).unbind(2)\n",
        "            w = AttentionOp.apply(q, k)\n",
        "            a = torch.einsum('nqk,nck->ncq', w, v)\n",
        "            x = self.proj(a.reshape(*x.shape)).add_(x)\n",
        "            x = x * self.skip_scale\n",
        "        return x\n",
        "\n",
        "#----------------------------------------------------------------------------\n",
        "# Timestep embedding used in the DDPM++ and ADM architectures.\n",
        "\n",
        "class PositionalEmbedding(torch.nn.Module):\n",
        "    def __init__(self, num_channels, max_positions=10000, endpoint=False):\n",
        "        super().__init__()\n",
        "        self.num_channels = num_channels\n",
        "        self.max_positions = max_positions\n",
        "        self.endpoint = endpoint\n",
        "\n",
        "    def forward(self, x):\n",
        "        freqs = torch.arange(start=0, end=self.num_channels//2, dtype=torch.float32, device=x.device)\n",
        "        freqs = freqs / (self.num_channels // 2 - (1 if self.endpoint else 0))\n",
        "        freqs = (1 / self.max_positions) ** freqs\n",
        "        x = x.ger(freqs.to(x.dtype))\n",
        "        x = torch.cat([x.cos(), x.sin()], dim=1)\n",
        "        return x\n",
        "\n",
        "#----------------------------------------------------------------------------\n",
        "# Timestep embedding used in the NCSN++ architecture.\n",
        "\n",
        "class FourierEmbedding(torch.nn.Module):\n",
        "    def __init__(self, num_channels, scale=16):\n",
        "        super().__init__()\n",
        "        self.register_buffer('freqs', torch.randn(num_channels // 2) * scale)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.ger((2 * np.pi * self.freqs).to(x.dtype))\n",
        "        x = torch.cat([x.cos(), x.sin()], dim=1)\n",
        "        return x\n",
        "\n",
        "#----------------------------------------------------------------------------\n",
        "# Reimplementation of the ADM architecture from the paper\n",
        "# \"Diffusion Models Beat GANS on Image Synthesis\". Equivalent to the\n",
        "# original implementation by Dhariwal and Nichol, available at\n",
        "# https://github.com/openai/guided-diffusion\n",
        "\n",
        "class UNet(torch.nn.Module):\n",
        "    def __init__(self,\n",
        "        img_resolution,                     # Image resolution at input/output.\n",
        "        in_channels,                        # Number of color channels at input.\n",
        "        out_channels,                       # Number of color channels at output.\n",
        "        label_dim           = 0,            # Number of class labels, 0 = unconditional.\n",
        "        augment_dim         = 0,            # Augmentation label dimensionality, 0 = no augmentation.\n",
        "\n",
        "        model_channels      = 128,          # Base multiplier for the number of\n",
        "                 # channels.\n",
        "        channel_mult        = [1,2,3,4],    # Per-resolution multipliers for the number of channels.\n",
        "        channel_mult_emb    = 4,            # Multiplier for the dimensionality of the embedding vector.\n",
        "        num_blocks          = 2,            # Number of residual blocks per resolution.\n",
        "        attn_resolutions    = [32,16,8],    # List of resolutions with self-attention.\n",
        "        dropout             = 0.10,         # List of resolutions with self-attention.\n",
        "        label_dropout       = 0,            # Dropout probability of class labels for classifier-free guidance.\n",
        "        use_diffuse = True                  # Use Unet for diffusion\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.label_dropout = label_dropout\n",
        "        emb_channels = model_channels * channel_mult_emb\n",
        "        init = dict(init_mode='kaiming_uniform', init_weight=np.sqrt(1/3), init_bias=np.sqrt(1/3))\n",
        "        init_zero = dict(init_mode='kaiming_uniform', init_weight=0, init_bias=0)\n",
        "        block_kwargs = dict(emb_channels=emb_channels, channels_per_head=64, dropout=dropout, init=init, init_zero=init_zero)\n",
        "\n",
        "        # Mapping.\n",
        "        self.map_noise = PositionalEmbedding(num_channels=model_channels) if use_diffuse else None\n",
        "        self.map_augment = Linear(in_features=augment_dim, out_features=model_channels, bias=False, **init_zero) if augment_dim else None\n",
        "        self.map_layer0 = Linear(in_features=model_channels, out_features=emb_channels, **init)\n",
        "        self.map_layer1 = Linear(in_features=emb_channels, out_features=emb_channels, **init)\n",
        "        self.map_label = Linear(in_features=label_dim, out_features=emb_channels, bias=False, init_mode='kaiming_normal', init_weight=np.sqrt(label_dim)) if label_dim else None\n",
        "\n",
        "        assert len(img_resolution) == 2\n",
        "\n",
        "        # Encoder.\n",
        "        self.enc = torch.nn.ModuleDict()\n",
        "        cout = in_channels\n",
        "        for level, mult in enumerate(channel_mult):\n",
        "            resx = img_resolution[0] >> level\n",
        "            resy = img_resolution[1] >> level\n",
        "            if level == 0:\n",
        "                cin = cout\n",
        "                cout = model_channels * mult\n",
        "                self.enc[f'{resx}x{resy}_conv'] = Conv2d(in_channels=cin, out_channels=cout, kernel=3, **init)\n",
        "            else:\n",
        "                self.enc[f'{resx}x{resy}_down'] = UNetBlock(in_channels=cout, out_channels=cout, down=True, **block_kwargs)\n",
        "            for idx in range(num_blocks):\n",
        "                cin = cout\n",
        "                cout = model_channels * mult\n",
        "                self.enc[f'{resx}x{resy}_block{idx}'] = UNetBlock(\n",
        "                    in_channels=cin, out_channels=cout, attention=(resx in\n",
        "                                                                   attn_resolutions), **block_kwargs)\n",
        "        skips = [block.out_channels for block in self.enc.values()]\n",
        "\n",
        "        # Decoder.\n",
        "        self.dec = torch.nn.ModuleDict()\n",
        "        for level, mult in reversed(list(enumerate(channel_mult))):\n",
        "            resx = img_resolution[0] >> level\n",
        "            resy = img_resolution[1] >> level\n",
        "            if level == len(channel_mult) - 1:\n",
        "                self.dec[f'{resx}x{resy}_in0'] = UNetBlock(in_channels=cout,\n",
        "                                                          out_channels=cout, attention=True, **block_kwargs)\n",
        "                self.dec[f'{resx}x{resy}_in1'] = UNetBlock(in_channels=cout,\n",
        "                                                          out_channels=cout, **block_kwargs)\n",
        "            else:\n",
        "                self.dec[f'{resx}x{resy}_up'] = UNetBlock(in_channels=cout,\n",
        "                                                         out_channels=cout, up=True, **block_kwargs)\n",
        "            for idx in range(num_blocks + 1):\n",
        "                cin = cout + skips.pop()\n",
        "                cout = model_channels * mult\n",
        "                self.dec[f'{resx}x{resy}_block{idx}'] = UNetBlock(\n",
        "                    in_channels=cin, out_channels=cout, attention=(resx in\n",
        "                                                                   attn_resolutions), **block_kwargs)\n",
        "        self.out_norm = GroupNorm(num_channels=cout)\n",
        "        self.out_conv = Conv2d(in_channels=cout, out_channels=out_channels, kernel=3, **init_zero)\n",
        "\n",
        "    def forward(self, x, noise_labels=None, class_labels=None,\n",
        "                augment_labels=None):\n",
        "        # Mapping.\n",
        "        emb = torch.zeros([1, self.map_layer1.in_features], device=x.device)\n",
        "        if self.map_label is not None:\n",
        "            tmp = class_labels\n",
        "            if self.training and self.label_dropout:\n",
        "                tmp = tmp * (torch.rand([x.shape[0], 1],\n",
        "                                        device=x.device) >= self.label_dropout).to(\n",
        "                    tmp.dtype)\n",
        "            emb = self.map_label(tmp)\n",
        "        if self.map_noise is not None:\n",
        "            emb_n = self.map_noise(noise_labels)\n",
        "            emb_n = silu(self.map_layer0(emb_n))\n",
        "            emb_n = self.map_layer1(emb_n)\n",
        "            emb = emb + emb_n\n",
        "        if self.map_augment is not None and augment_labels is not None:\n",
        "            emb = emb + self.map_augment(augment_labels)\n",
        "\n",
        "        emb = silu(emb)\n",
        "\n",
        "        # Encoder.\n",
        "        skips = []\n",
        "        for block in self.enc.values():\n",
        "            x = block(x, emb) if isinstance(block, UNetBlock) else block(x)\n",
        "            skips.append(x)\n",
        "\n",
        "        # Decoder.\n",
        "        for block in self.dec.values():\n",
        "            if x.shape[1] != block.in_channels:\n",
        "                x = torch.cat([x, skips.pop()], dim=1)\n",
        "            x = block(x, emb)\n",
        "        x = self.out_conv(silu(self.out_norm(x)))\n",
        "        return x\n",
        "\n",
        "#----------------------------------------------------------------------------\n",
        "# Improved preconditioning proposed in the paper \"Elucidating the Design\n",
        "# Space of Diffusion-Based Generative Models\" (EDM).\n",
        "\n",
        "class EDMPrecond(torch.nn.Module):\n",
        "    def __init__(self,\n",
        "        img_resolution,                     # Image resolution.\n",
        "        in_channels,                       # Number of color channels.\n",
        "        out_channels,                       # Number of color channels.\n",
        "        label_dim       = 0,                # Number of class labels, 0 = unconditional.\n",
        "        use_fp16        = False,            # Execute the underlying model at FP16 precision?\n",
        "        sigma_min       = 0,                # Minimum supported noise level.\n",
        "        sigma_max       = float('inf'),     # Maximum supported noise level.\n",
        "        sigma_data      = 1.0,              # Expected standard deviation of\n",
        "                 # the training data.\n",
        "        model_type      = 'UNet',   # Class name of the underlying model.\n",
        "        **model_kwargs,                     # Keyword arguments for the underlying model.\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.img_resolution = img_resolution\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.label_dim = label_dim\n",
        "        self.use_fp16 = use_fp16\n",
        "        self.sigma_min = sigma_min\n",
        "        self.sigma_max = sigma_max\n",
        "        self.sigma_data = sigma_data\n",
        "        self.model = globals()[model_type](\n",
        "            img_resolution=img_resolution, in_channels=in_channels,\n",
        "            out_channels=out_channels, label_dim=label_dim, **model_kwargs)\n",
        "\n",
        "    def forward(self, x, sigma, condition_img=None, class_labels=None,\n",
        "                force_fp32=True, **model_kwargs):\n",
        "        if condition_img is not None:\n",
        "            in_img = torch.cat([x, condition_img], dim=1)\n",
        "        else:\n",
        "            in_img = x\n",
        "        sigma = sigma.reshape(-1, 1, 1, 1)\n",
        "        class_labels = None if self.label_dim == 0 else torch.zeros([1, self.label_dim], device=in_img.device) if class_labels is None else class_labels.to(torch.float32).reshape(-1, self.label_dim)\n",
        "        dtype = torch.float16 if (self.use_fp16 and not force_fp32 and in_img.device.type == 'cuda') else torch.float32\n",
        "\n",
        "        c_skip = self.sigma_data ** 2 / (sigma ** 2 + self.sigma_data ** 2)\n",
        "        c_out = sigma * self.sigma_data / (sigma ** 2 + self.sigma_data ** 2).sqrt()\n",
        "        c_in = 1 / (self.sigma_data ** 2 + sigma ** 2).sqrt()\n",
        "        c_noise = sigma.log() / 4\n",
        "\n",
        "        F_x = self.model((c_in * in_img).to(dtype),\n",
        "                         noise_labels=c_noise.flatten(),\n",
        "                         class_labels=class_labels, **model_kwargs).to(dtype)\n",
        "        assert F_x.dtype == dtype\n",
        "        D_x = c_skip * x + c_out * F_x\n",
        "        return D_x\n",
        "\n",
        "    def round_sigma(self, sigma):\n",
        "        return torch.as_tensor(sigma)\n",
        "\n",
        "#----------------------------------------------------------------------------"
      ],
      "metadata": {
        "id": "2Sje388j2P_J"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test whether input and output of UNET have same shape"
      ],
      "metadata": {
        "id": "Dblaa-bB2WnB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Define the input shape\n",
        "batch_size = 4\n",
        "in_channels = 3\n",
        "img_resolution = (128, 128)  # Height and Width of the image\n",
        "\n",
        "# Create a random input tensor\n",
        "input_tensor = torch.randn(batch_size, in_channels, *img_resolution)\n",
        "\n",
        "# Define the UNet model\n",
        "model = UNet(\n",
        "    img_resolution=img_resolution,\n",
        "    in_channels=in_channels,\n",
        "    out_channels=in_channels,  # Assuming the output should have the same number of channels as input\n",
        "    label_dim=0,\n",
        "    augment_dim=0,\n",
        "    model_channels=128,\n",
        "    channel_mult=[1, 2, 3, 4],\n",
        "    channel_mult_emb=4,\n",
        "    num_blocks=2,\n",
        "    attn_resolutions=[32, 16, 8],\n",
        "    dropout=0.10,\n",
        "    label_dropout=0,\n",
        "    use_diffuse=True\n",
        ")\n",
        "\n",
        "# Move model and input to the same device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "input_tensor = input_tensor.to(device)\n",
        "\n",
        "# Forward pass through the model\n",
        "output_tensor = model(input_tensor, noise_labels=torch.ones(batch_size, device=device))\n",
        "\n",
        "# Print the shapes of the input and output tensors\n",
        "print(f\"Input shape: {input_tensor.shape}\")\n",
        "print(f\"Output shape: {output_tensor.shape}\")\n",
        "\n",
        "# Assert that the input and output shapes match\n",
        "assert input_tensor.shape == output_tensor.shape, \"Input and output shapes do not match!\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pmAaxNo02Q0i",
        "outputId": "8cb16f5f-eaba-43dc-d450-56a60be4c1c3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input shape: torch.Size([4, 3, 128, 128])\n",
            "Output shape: torch.Size([4, 3, 128, 128])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test for EDMPrecond"
      ],
      "metadata": {
        "id": "IQTqKIej4Dla"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Define the input shape\n",
        "batch_size = 4\n",
        "in_channels = 3\n",
        "img_resolution = (128, 128)  # Height and Width of the image\n",
        "\n",
        "# Create a random input tensor\n",
        "input_tensor = torch.randn(batch_size, in_channels, *img_resolution)\n",
        "\n",
        "# Define the UNet model\n",
        "unet_model = UNet(\n",
        "    img_resolution=img_resolution,\n",
        "    in_channels=in_channels,\n",
        "    out_channels=in_channels,  # Assuming the output should have the same number of channels as input\n",
        "    label_dim=0,\n",
        "    augment_dim=0,\n",
        "    model_channels=128,\n",
        "    channel_mult=[1, 2, 3, 4],\n",
        "    channel_mult_emb=4,\n",
        "    num_blocks=2,\n",
        "    attn_resolutions=[32, 16, 8],\n",
        "    dropout=0.10,\n",
        "    label_dropout=0,\n",
        "    use_diffuse=True\n",
        ")\n",
        "\n",
        "# Define the EDMPrecond model\n",
        "edm_precond_model = EDMPrecond(\n",
        "    img_resolution=img_resolution,\n",
        "    in_channels=in_channels,\n",
        "    out_channels=in_channels,  # Assuming the output should have the same number of channels as input\n",
        "    label_dim=0,\n",
        "    use_fp16=False,\n",
        "    sigma_min=0,\n",
        "    sigma_max=float('inf'),\n",
        "    sigma_data=1.0,\n",
        "    model_type='UNet',\n",
        "    model_channels=128,\n",
        "    channel_mult=[1, 2, 3, 4],\n",
        "    channel_mult_emb=4,\n",
        "    num_blocks=2,\n",
        "    attn_resolutions=[32, 16, 8],\n",
        "    dropout=0.10,\n",
        "    label_dropout=0,\n",
        "    use_diffuse=True\n",
        ")\n",
        "\n",
        "# Move models and input to the same device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "unet_model.to(device)\n",
        "edm_precond_model.to(device)\n",
        "input_tensor = input_tensor.to(device)\n",
        "\n",
        "# Create noise labels (assuming a simple case with all ones)\n",
        "noise_labels = torch.ones(batch_size, device=device)\n",
        "\n",
        "# Forward pass through the UNet model\n",
        "unet_output_tensor = unet_model(input_tensor, noise_labels=noise_labels)\n",
        "\n",
        "# Print the shapes of the input and output tensors for UNet\n",
        "print(f\"UNet Input shape: {input_tensor.shape}\")\n",
        "print(f\"UNet Output shape: {unet_output_tensor.shape}\")\n",
        "\n",
        "# Assert that the input and output shapes match for UNet\n",
        "assert input_tensor.shape == unet_output_tensor.shape, \"UNet input and output shapes do not match!\"\n",
        "\n",
        "# Create sigma (noise level) tensor for EDMPrecond\n",
        "sigma = torch.ones(batch_size, device=device)\n",
        "\n",
        "# Forward pass through the EDMPrecond model\n",
        "edm_output_tensor = edm_precond_model(input_tensor, sigma=sigma)\n",
        "\n",
        "# Print the shapes of the input and output tensors for EDMPrecond\n",
        "print(f\"EDMPrecond Input shape: {input_tensor.shape}\")\n",
        "print(f\"EDMPrecond Output shape: {edm_output_tensor.shape}\")\n",
        "\n",
        "# Assert that the input and output shapes match for EDMPrecond\n",
        "assert input_tensor.shape == edm_output_tensor.shape, \"EDMPrecond input and output shapes do not match!\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vRKAIlTv2aoT",
        "outputId": "8befd198-8bb7-44e7-9d8d-c0ddeffdf963"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "UNet Input shape: torch.Size([4, 3, 128, 128])\n",
            "UNet Output shape: torch.Size([4, 3, 128, 128])\n",
            "EDMPrecond Input shape: torch.Size([4, 3, 128, 128])\n",
            "EDMPrecond Output shape: torch.Size([4, 3, 128, 128])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "\n",
        "# Loss class taken from EDS_Diffusion/loss.py\n",
        "class EDMLoss:\n",
        "    def __init__(self, P_mean=-1.2, P_std=1.2, sigma_data=1.0):\n",
        "        self.P_mean = P_mean\n",
        "        self.P_std = P_std\n",
        "        self.sigma_data = sigma_data\n",
        "\n",
        "    def __call__(self, net, images, images_target):\n",
        "        rnd_normal = torch.randn([images.shape[0], 1, 1, 1], device=images.device)\n",
        "        sigma = (rnd_normal * self.P_std + self.P_mean).exp()\n",
        "        weight = (sigma ** 2 + self.sigma_data ** 2) / (sigma * self.sigma_data)**2\n",
        "        y = images_target\n",
        "        n = torch.randn_like(y) * sigma\n",
        "        D_yn = net(y + n, sigma)\n",
        "        loss = weight * ((D_yn - y) ** 2)\n",
        "        return loss.mean()"
      ],
      "metadata": {
        "id": "t5OGvOUY4Ifi"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.nn.functional import silu\n",
        "\n",
        "# Dummy dataset with random data\n",
        "class RandomDataset(Dataset):\n",
        "    def __init__(self, num_samples, img_shape):\n",
        "        self.num_samples = num_samples\n",
        "        self.img_shape = img_shape\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_samples\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {\n",
        "            \"inputs\": torch.randn(*self.img_shape),\n",
        "            \"targets\": torch.randn(*self.img_shape)\n",
        "        }"
      ],
      "metadata": {
        "id": "-ZwBzZmQ5A-K"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training function\n",
        "def training_step(model, loss_fn, optimizer, data_loader, device=\"cuda\"):\n",
        "    model.train()\n",
        "    epoch_losses = []\n",
        "    for batch in data_loader:\n",
        "        image_input = batch[\"inputs\"].to(device)\n",
        "        images_target = batch[\"targets\"].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss = loss_fn(net=model, images=image_input, images_target=images_target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_losses.append(loss.item())\n",
        "    return sum(epoch_losses) / len(epoch_losses)"
      ],
      "metadata": {
        "id": "lY04gXor5HJq"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 8\n",
        "num_epochs = 10000\n",
        "img_shape = (3, 64, 64)\n",
        "num_samples = 100\n",
        "\n",
        "# Define device\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model = EDMPrecond(img_resolution=img_shape[1:], in_channels=img_shape[0], out_channels=img_shape[0])\n",
        "model.to(device)\n",
        "\n",
        "# Create random dataset\n",
        "dataset = RandomDataset(num_samples, img_shape)\n",
        "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "loss_fn = EDMLoss()\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
        "\n",
        "# Train the model\n",
        "for epoch in range(num_epochs):\n",
        "    epoch_loss = training_step(model, loss_fn, optimizer, data_loader, device)\n",
        "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {epoch_loss:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U0YrsaWn4xFC",
        "outputId": "e71ccc67-f992-4f21-dd3c-d5ed0cf71517"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10000, Loss: 1.0017\n",
            "Epoch 2/10000, Loss: 0.9994\n",
            "Epoch 3/10000, Loss: 0.9978\n",
            "Epoch 4/10000, Loss: 1.0002\n",
            "Epoch 5/10000, Loss: 0.9999\n",
            "Epoch 6/10000, Loss: 1.0018\n",
            "Epoch 7/10000, Loss: 0.9985\n",
            "Epoch 8/10000, Loss: 1.0031\n",
            "Epoch 9/10000, Loss: 0.9995\n",
            "Epoch 10/10000, Loss: 0.9992\n",
            "Epoch 11/10000, Loss: 0.9989\n",
            "Epoch 12/10000, Loss: 0.9988\n",
            "Epoch 13/10000, Loss: 1.0008\n",
            "Epoch 14/10000, Loss: 0.9987\n",
            "Epoch 15/10000, Loss: 0.9992\n",
            "Epoch 16/10000, Loss: 1.0026\n",
            "Epoch 17/10000, Loss: 0.9980\n",
            "Epoch 18/10000, Loss: 1.0024\n",
            "Epoch 19/10000, Loss: 0.9987\n",
            "Epoch 20/10000, Loss: 0.9992\n",
            "Epoch 21/10000, Loss: 0.9992\n",
            "Epoch 22/10000, Loss: 1.0024\n",
            "Epoch 23/10000, Loss: 1.0007\n",
            "Epoch 24/10000, Loss: 0.9997\n",
            "Epoch 25/10000, Loss: 0.9995\n",
            "Epoch 26/10000, Loss: 1.0033\n",
            "Epoch 27/10000, Loss: 1.0019\n",
            "Epoch 28/10000, Loss: 1.0009\n",
            "Epoch 29/10000, Loss: 1.0002\n",
            "Epoch 30/10000, Loss: 1.0013\n",
            "Epoch 31/10000, Loss: 0.9989\n",
            "Epoch 32/10000, Loss: 1.0014\n",
            "Epoch 33/10000, Loss: 0.9976\n",
            "Epoch 34/10000, Loss: 0.9997\n",
            "Epoch 35/10000, Loss: 1.0015\n",
            "Epoch 36/10000, Loss: 1.0003\n",
            "Epoch 37/10000, Loss: 1.0009\n",
            "Epoch 38/10000, Loss: 0.9993\n",
            "Epoch 39/10000, Loss: 1.0002\n",
            "Epoch 40/10000, Loss: 1.0014\n",
            "Epoch 41/10000, Loss: 0.9997\n",
            "Epoch 42/10000, Loss: 1.0002\n",
            "Epoch 43/10000, Loss: 1.0005\n",
            "Epoch 44/10000, Loss: 1.0000\n",
            "Epoch 45/10000, Loss: 0.9993\n",
            "Epoch 46/10000, Loss: 1.0002\n",
            "Epoch 47/10000, Loss: 0.9998\n",
            "Epoch 48/10000, Loss: 1.0006\n",
            "Epoch 49/10000, Loss: 1.0001\n",
            "Epoch 50/10000, Loss: 1.0027\n",
            "Epoch 51/10000, Loss: 1.0021\n",
            "Epoch 52/10000, Loss: 0.9999\n",
            "Epoch 53/10000, Loss: 0.9996\n",
            "Epoch 54/10000, Loss: 1.0025\n",
            "Epoch 55/10000, Loss: 0.9995\n",
            "Epoch 56/10000, Loss: 0.9987\n",
            "Epoch 57/10000, Loss: 1.0008\n",
            "Epoch 58/10000, Loss: 1.0012\n",
            "Epoch 59/10000, Loss: 0.9982\n",
            "Epoch 60/10000, Loss: 0.9982\n",
            "Epoch 61/10000, Loss: 0.9987\n",
            "Epoch 62/10000, Loss: 0.9994\n",
            "Epoch 63/10000, Loss: 1.0023\n",
            "Epoch 64/10000, Loss: 0.9999\n",
            "Epoch 65/10000, Loss: 1.0026\n",
            "Epoch 66/10000, Loss: 0.9986\n",
            "Epoch 67/10000, Loss: 1.0006\n",
            "Epoch 68/10000, Loss: 0.9999\n",
            "Epoch 69/10000, Loss: 1.0007\n",
            "Epoch 70/10000, Loss: 0.9997\n",
            "Epoch 71/10000, Loss: 1.0002\n",
            "Epoch 72/10000, Loss: 1.0001\n",
            "Epoch 73/10000, Loss: 0.9984\n",
            "Epoch 74/10000, Loss: 0.9993\n",
            "Epoch 75/10000, Loss: 0.9979\n",
            "Epoch 76/10000, Loss: 1.0006\n",
            "Epoch 77/10000, Loss: 1.0003\n",
            "Epoch 78/10000, Loss: 0.9993\n",
            "Epoch 79/10000, Loss: 0.9991\n",
            "Epoch 80/10000, Loss: 1.0008\n",
            "Epoch 81/10000, Loss: 0.9997\n",
            "Epoch 82/10000, Loss: 1.0013\n",
            "Epoch 83/10000, Loss: 0.9994\n",
            "Epoch 84/10000, Loss: 0.9996\n",
            "Epoch 85/10000, Loss: 1.0012\n",
            "Epoch 86/10000, Loss: 1.0008\n",
            "Epoch 87/10000, Loss: 1.0002\n",
            "Epoch 88/10000, Loss: 1.0024\n",
            "Epoch 89/10000, Loss: 0.9992\n",
            "Epoch 90/10000, Loss: 0.9981\n",
            "Epoch 91/10000, Loss: 1.0010\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iUDsEUc17C5Z"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}